manifest {
  name = 'scpca-nf'
  description = 'A nextflow workflow for processing single-cell RNA-seq data as part of the ScPCA project.'
  homePage = 'https://github.com/AlexsLemonade/scpca-nf'
  doi = '10.1101/2024.04.19.590243'
  mainScript = 'main.nf'
  defaultBranch = 'main'
  nextflowVersion = '>=24.09'
  version = 'v0.8.5'
  contributors = [
    {
      name = "Allegra Hawkins"
      affiliation = "Alex's Lemonade Stand Foundation"
      contribution = "author"
      github = "https://github.com/allyhawkins"
      orcid = "https://orcid.org/0000-0001-6026-3660"

    },
    {
      name = "Joshua A. Shapiro"
      affiliation = "Alex's Lemonade Stand Foundation"
      contribution = "author"
      github = "https://github.com/jashapiro"
      orcid = "https://orcid.org/0000-0002-6224-0347"
    },
    {
      name = "Stephanie J. Spielman"
      affiliation = "Alex's Lemonade Stand Foundation"
      contribution = "author"
      github = "https://github.com/sjspielman"
      orcid = "https://orcid.org/0000-0002-9090-4788"
    }
  ]
}

// global parameters for workflows
params {
  // Input data table
  run_metafile = "run_metadata.tsv"
  sample_metafile = "sample_metadata.tsv"

  // Output base directory
  outdir = "scpca_out"

  // create paths to output folders
  checkpoints_dir = "${params.outdir}/checkpoints"
  results_dir = "${params.outdir}/results"

  // File containing cellhash pool sample/antibody descriptions
  cellhash_pool_file = "multiplex_pools.tsv"

  // run_ids are comma separated list to be parsed into
  // a list of run ids, library ids, and or sample_ids
  // or "All" to process all samples in the metadata file
  run_ids = "All"
  // to run all samples in a project use that project's submitter name
  project = null
  // if running the merge workflow, include all runs in a project by default
  merge_run_ids = "All"

  // Processing options
  af_resolution = 'cr-like-em' // alevin-fry quant resolution method: default is cr-like, can also use full, cr-like-em, parsimony, and trivial
  repeat_mapping = false // if alevin or salmon mapping has already been performed and output files exist, mapping is skipped by default. Use `--repeat_mapping` to perform mapping again
  repeat_genetic_demux = false // if genetic demultiplexing has been performed and output files exist, genetic demux is skipped by default. Use `--repeat_genetic_demux` to run these steps.
  skip_genetic_demux = false // skip genetic demultiplexing steps, even if bulk data is present
  publish_fry_outs = false // alevin-fry outputs are not published by default. Use `--publish_fry_outs` to publish these files to the `checkpoints` folder.
  spliced_only = false // include only spliced reads in counts matrix, by default both unspliced and spliced reads are totaled and found in `counts` asasy of returned SingleCellExperiment object
  perform_celltyping = false // specify whether or not to incorporate cell type annotations
  repeat_celltyping = false // if cell type annotations already exist, skip cell type classification with SingleR and CellAssign

  seed = 2021   // random number seed for filtering and post-processing (0 means use system seed)

  // post processing SCE options
  prob_compromised_cutoff = 0.75 // probability compromised cutoff used for filtering cells with miQC
  gene_cutoff = 200 // minimum number of genes per cell cutoff used for filtering cells
  num_hvg = 2000 // number of high variance genes to use for dimension reduction
  num_pcs = 50 // number of principal components to retain in the returned SingleCellExperiment object

  // SCE clustering options
  cluster_algorithm = "louvain" // default graph-based clustering algorithm
  cluster_weighting = "jaccard" // default weighting scheme for graph-based clustering
  nearest_neighbors = 20 // default nearest neighbors parameter for graph-based clustering

  // Cell type annotation options
  singler_label_name = "label.ont" // celldex reference label used for SingleR reference building

  // Merge workflow-specfic options
  params.reuse_merge = false // if later steps fail, you can use `--reuse_merge` reuse the merged RDS object during a rerun
  params.max_merge_libraries = 100 // maximum number of libraries that can be merged

  // Docker container images
  includeConfig 'config/containers.config'

  // reference and annotation parameters
  includeConfig 'config/reference_paths.config'

}

// Load base process config with labels
includeConfig 'config/process_base.config'

profiles {
  standard {
    process.executor = 'local'
    docker.enabled = true
    docker.fixOwnership = true
  }
  // AWS batch profile with autoscaling disk
  batch {
    includeConfig 'config/profile_awsbatch_auto.config'
  }
  // AWS batch profile with manual disk sizing
  batch_manual {
    includeConfig 'config/profile_awsbatch.config'
  }
  // CCDL-specific settings
  ccdl {
    includeConfig 'config/profile_ccdl.config'
  }
  // stub testing profile
  stub {
    includeConfig 'config/profile_stub.config'
  }
  // settings to process example data
  example {
    includeConfig 'config/profile_example.config'
  }
}
